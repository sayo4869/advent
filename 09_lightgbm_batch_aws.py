"""
LightGBM AWS ECSç”¨ãƒãƒƒãƒäºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã€œS3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾— â†’ ç‰¹å¾´é‡ç”Ÿæˆ â†’ äºˆæ¸¬ â†’ S3ã«ä¿å­˜ã€œ

ç’°å¢ƒå¤‰æ•°:
    S3_INPUT_BUCKET: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®S3ãƒã‚±ãƒƒãƒˆ
    S3_INPUT_KEY: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®S3ã‚­ãƒ¼
    S3_OUTPUT_BUCKET: å‡ºåŠ›å…ˆã®S3ãƒã‚±ãƒƒãƒˆ
    S3_OUTPUT_PREFIX: å‡ºåŠ›å…ˆã®S3ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    S3_MODEL_BUCKET: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®S3ãƒã‚±ãƒƒãƒˆ
    S3_MODEL_KEY: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®S3ã‚­ãƒ¼
    PREDICTION_DAYS: äºˆæ¸¬æ—¥æ•°
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Tuple
import lightgbm as lgb
import json
import logging
import boto3
from io import StringIO, BytesIO
import pickle

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LightGBMForecastBatchAWS:
    """LightGBM AWSç”¨å£²ä¸Šäºˆæ¸¬ãƒãƒƒãƒå‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
        self.s3_input_bucket = os.environ.get('S3_INPUT_BUCKET', 'my-forecast-bucket')
        self.s3_input_key = os.environ.get('S3_INPUT_KEY', 'data/retail_sales_preprocessed.csv')
        self.s3_output_bucket = os.environ.get('S3_OUTPUT_BUCKET', 'my-forecast-bucket')
        self.s3_output_prefix = os.environ.get('S3_OUTPUT_PREFIX', 'forecasts/')
        self.s3_model_bucket = os.environ.get('S3_MODEL_BUCKET', 'my-forecast-bucket')
        self.s3_model_key = os.environ.get('S3_MODEL_KEY', 'models/lightgbm_model.pkl')
        self.prediction_days = int(os.environ.get('PREDICTION_DAYS', '30'))

        self.s3_client = boto3.client('s3')
        self.model = None
        self.feature_cols = None
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")

    def load_model_from_s3(self):
        """S3ã‹ã‚‰LightGBMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        logger.info(f"ğŸ¤– Loading LightGBM model from s3://{self.s3_model_bucket}/{self.s3_model_key}...")

        response = self.s3_client.get_object(
            Bucket=self.s3_model_bucket,
            Key=self.s3_model_key
        )

        model_data = pickle.loads(response['Body'].read())
        self.model = model_data['model']
        self.feature_cols = model_data['feature_cols']

        logger.info(f"   âœ… Model loaded ({len(self.feature_cols)} features)")

    def load_data_from_s3(self) -> pd.DataFrame:
        """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ“‚ Loading data from s3://{self.s3_input_bucket}/{self.s3_input_key}...")

        response = self.s3_client.get_object(
            Bucket=self.s3_input_bucket,
            Key=self.s3_input_key
        )

        df = pd.read_csv(response['Body'])
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)

        if 'sales' not in df.columns:
            raise ValueError("'sales' column not found in data")

        logger.info(f"   âœ… Loaded {len(df)} records")
        logger.info(f"   ğŸ“… Latest date: {df['date'].max()}")

        return df

    def create_lag_features(self, df: pd.DataFrame, lag_days: List[int] = None) -> pd.DataFrame:
        """ãƒ©ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        if lag_days is None:
            lag_days = [1, 2, 3, 4, 5, 6, 7, 14, 21, 28]

        df = df.copy()
        for lag in lag_days:
            df[f'lag_{lag}'] = df['sales'].shift(lag)
        return df

    def create_rolling_features(self, df: pd.DataFrame, windows: List[int] = None) -> pd.DataFrame:
        """ãƒ­ãƒ¼ãƒªãƒ³ã‚°ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        if windows is None:
            windows = [7, 14, 28]

        df = df.copy()
        for window in windows:
            shifted = df['sales'].shift(1)
            df[f'rolling_mean_{window}'] = shifted.rolling(window=window, min_periods=1).mean()
            df[f'rolling_std_{window}'] = shifted.rolling(window=window, min_periods=1).std()
            df[f'rolling_max_{window}'] = shifted.rolling(window=window, min_periods=1).max()
            df[f'rolling_min_{window}'] = shifted.rolling(window=window, min_periods=1).min()
        return df

    def create_date_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ—¥ä»˜ç‰¹å¾´é‡ã‚’ä½œæˆ"""
        df = df.copy()

        df['day_of_week'] = df['date'].dt.dayofweek
        df['month'] = df['date'].dt.month
        df['day'] = df['date'].dt.day
        df['day_of_year'] = df['date'].dt.dayofyear
        df['week_of_year'] = df['date'].dt.isocalendar().week.astype(int)
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        df['week_of_month'] = (df['day'] - 1) // 7 + 1
        df['season'] = df['month'].map({
            1: 0, 2: 0, 3: 1, 4: 1, 5: 1, 6: 2,
            7: 2, 8: 2, 9: 3, 10: 3, 11: 3, 12: 0
        })

        # ã‚µã‚¤ãƒ³ãƒ»ã‚³ã‚µã‚¤ãƒ³å¤‰æ›
        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
        df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)

        return df

    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å…¨ç‰¹å¾´é‡ã‚’æº–å‚™"""
        df = self.create_date_features(df)
        df = self.create_lag_features(df)
        df = self.create_rolling_features(df)
        return df

    def predict_recursive(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        å†å¸°çš„ã«äºˆæ¸¬ï¼ˆ1æ—¥ãšã¤äºˆæ¸¬ã—ã¦ç‰¹å¾´é‡ã‚’æ›´æ–°ï¼‰

        âš ï¸ ãƒã‚¤ãƒ³ãƒˆ: è¤‡æ•°æ—¥äºˆæ¸¬ã™ã‚‹å ´åˆã€å‰æ—¥ã®äºˆæ¸¬å€¤ã‚’ä½¿ã£ã¦
        æ¬¡ã®æ—¥ã®ç‰¹å¾´é‡ã‚’ä½œã‚‹å¿…è¦ãŒã‚ã‚‹
        """
        logger.info(f"ğŸ”® Predicting next {self.prediction_days} days (recursive)...")

        df = df.copy()
        last_date = df['date'].max()
        predictions = []

        for day in range(1, self.prediction_days + 1):
            # æ¬¡ã®æ—¥ã®æ—¥ä»˜
            next_date = last_date + pd.Timedelta(days=day)

            # æ–°ã—ã„è¡Œã‚’è¿½åŠ ï¼ˆsalesã¯NaNï¼‰
            new_row = pd.DataFrame({'date': [next_date], 'sales': [np.nan]})
            df = pd.concat([df, new_row], ignore_index=True)

            # ç‰¹å¾´é‡ã‚’å†è¨ˆç®—
            df = self.prepare_features(df)

            # æœ€å¾Œã®è¡Œã§äºˆæ¸¬
            X = df[self.feature_cols].iloc[-1:].fillna(0)
            pred = self.model.predict(X)[0]

            # äºˆæ¸¬å€¤ã‚’salesã«è¨­å®šï¼ˆæ¬¡ã®æ—¥ã®ãƒ©ã‚°ç‰¹å¾´é‡ã«ä½¿ã†ï¼‰
            df.loc[df.index[-1], 'sales'] = pred

            predictions.append({
                'date': next_date,
                'forecast': int(pred),
            })

            if day % 10 == 0:
                logger.info(f"   ... {day}/{self.prediction_days} days completed")

        results = pd.DataFrame(predictions)
        logger.info("   âœ… Prediction completed")
        logger.info(f"   ğŸ“Š Forecast mean: Â¥{results['forecast'].mean():,.0f}")

        return results

    def save_results_to_s3(self, results: pd.DataFrame):
        """çµæœã‚’S3ã«ä¿å­˜"""
        # CSVä¿å­˜
        csv_key = f"{self.s3_output_prefix}forecast_{self.run_id}.csv"
        csv_buffer = StringIO()
        results.to_csv(csv_buffer, index=False)

        self.s3_client.put_object(
            Bucket=self.s3_output_bucket,
            Key=csv_key,
            Body=csv_buffer.getvalue(),
            ContentType='text/csv'
        )
        logger.info(f"ğŸ’¾ Saved forecast to s3://{self.s3_output_bucket}/{csv_key}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            'run_id': self.run_id,
            'model': 'LightGBM',
            'prediction_days': self.prediction_days,
            'created_at': datetime.now().isoformat(),
            'forecast_start': results['date'].min().isoformat(),
            'forecast_end': results['date'].max().isoformat(),
            'forecast_mean': float(results['forecast'].mean()),
            'forecast_total': float(results['forecast'].sum()),
            's3_input': f"s3://{self.s3_input_bucket}/{self.s3_input_key}",
            's3_model': f"s3://{self.s3_model_bucket}/{self.s3_model_key}",
            's3_output': f"s3://{self.s3_output_bucket}/{csv_key}",
        }

        meta_key = f"{self.s3_output_prefix}metadata_{self.run_id}.json"
        self.s3_client.put_object(
            Bucket=self.s3_output_bucket,
            Key=meta_key,
            Body=json.dumps(metadata, indent=2, ensure_ascii=False),
            ContentType='application/json'
        )
        logger.info(f"ğŸ’¾ Saved metadata to s3://{self.s3_output_bucket}/{meta_key}")

        # latest.jsonæ›´æ–°
        latest = {
            'latest_run_id': self.run_id,
            'latest_forecast': f"s3://{self.s3_output_bucket}/{csv_key}",
            'updated_at': datetime.now().isoformat(),
        }
        self.s3_client.put_object(
            Bucket=self.s3_output_bucket,
            Key=f"{self.s3_output_prefix}latest.json",
            Body=json.dumps(latest, indent=2),
            ContentType='application/json'
        )

        return csv_key, meta_key

    def run(self) -> dict:
        """ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
        start_time = datetime.now()

        logger.info("=" * 60)
        logger.info("ğŸš€ Starting LightGBM batch forecast job")
        logger.info(f"   Run ID: {self.run_id}")
        logger.info("=" * 60)

        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            self.load_model_from_s3()

            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.load_data_from_s3()

            # äºˆæ¸¬ï¼ˆå†å¸°çš„ï¼‰
            results = self.predict_recursive(df)

            # S3ã«ä¿å­˜
            csv_key, meta_key = self.save_results_to_s3(results)

            elapsed = (datetime.now() - start_time).total_seconds()

            logger.info("=" * 60)
            logger.info(f"âœ… Batch job completed successfully")
            logger.info(f"â±ï¸ Elapsed time: {elapsed:.1f} seconds")
            logger.info("=" * 60)

            return {
                'statusCode': 200,
                'run_id': self.run_id,
                'forecast_count': len(results),
                'forecast_mean': float(results['forecast'].mean()),
                's3_output': f"s3://{self.s3_output_bucket}/{csv_key}",
                'elapsed_seconds': elapsed,
            }

        except Exception as e:
            logger.error(f"âŒ Batch job failed: {e}")
            import traceback
            traceback.print_exc()
            return {
                'statusCode': 500,
                'error': str(e),
                'run_id': self.run_id,
            }


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    batch = LightGBMForecastBatchAWS()
    result = batch.run()

    print(json.dumps(result, indent=2))

    if result.get('statusCode') != 200:
        exit(1)


if __name__ == "__main__":
    main()
