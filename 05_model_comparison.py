"""
å°å£²å£²ä¸Šäºˆæ¸¬ã®ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ & ç²¾åº¦è©•ä¾¡
ã€œã©ã®ãƒ¢ãƒ‡ãƒ«ãŒä¸€ç•ªã‚¤ã‚±ã¦ã‚‹ã‹æ±ºç€ã‚’ã¤ã‘ã‚‹ã€œ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from typing import Dict
import warnings
import japanize_matplotlib

warnings.filterwarnings('ignore')


def mean_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """MAPEï¼ˆå¹³å‡çµ¶å¯¾ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆèª¤å·®ï¼‰ã‚’è¨ˆç®—"""
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    mask = y_true != 0
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100


def load_predictions() -> Dict[str, pd.DataFrame]:
    """å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã‚€"""
    predictions = {}

    # Prophet
    prophet_df = pd.read_csv("prophet_predictions.csv")
    prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])
    prophet_df = prophet_df.rename(columns={'ds': 'date', 'y': 'actual', 'yhat': 'prediction'})
    predictions['Prophet'] = prophet_df

    # LightGBM
    lgb_df = pd.read_csv("lightgbm_predictions.csv")
    lgb_df['date'] = pd.to_datetime(lgb_df['date'])
    lgb_df = lgb_df.rename(columns={'sales': 'actual'})
    predictions['LightGBM'] = lgb_df

    return predictions


def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    """è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—"""
    return {
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'MAE': mean_absolute_error(y_true, y_pred),
        'MAPE': mean_absolute_percentage_error(y_true, y_pred),
        'R2': r2_score(y_true, y_pred)
    }


def compare_models(predictions: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    """ãƒ¢ãƒ‡ãƒ«é–“ã®æ¯”è¼ƒ"""
    print("=" * 60)
    print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ")
    print("=" * 60)

    results = []

    for model_name, df in predictions.items():
        metrics = calculate_metrics(df['actual'], df['prediction'])
        metrics['model'] = model_name
        results.append(metrics)

        print(f"\nã€{model_name}ã€‘")
        print(f"   RMSE: Â¥{metrics['RMSE']:,.0f}")
        print(f"   MAE:  Â¥{metrics['MAE']:,.0f}")
        print(f"   MAPE: {metrics['MAPE']:.2f}%")
        print(f"   RÂ²:   {metrics['R2']:.4f}")

    results_df = pd.DataFrame(results)
    results_df = results_df[['model', 'RMSE', 'MAE', 'MAPE', 'R2']]

    return results_df


def plot_comparison(
    predictions: Dict[str, pd.DataFrame],
    metrics_df: pd.DataFrame,
    save_path: str = "figures/"
) -> None:
    """æ¯”è¼ƒçµæœã‚’ãƒ—ãƒ­ãƒƒãƒˆ"""
    import os
    os.makedirs(save_path, exist_ok=True)

    # === 1. äºˆæ¸¬çµæœã®æ™‚ç³»åˆ—æ¯”è¼ƒ ===
    fig, ax = plt.subplots(figsize=(14, 6))

    # å®Ÿç¸¾ï¼ˆã©ã¡ã‚‰ã®DataFrameã§ã‚‚åŒã˜ã¯ãšï¼‰
    first_df = list(predictions.values())[0]
    ax.plot(first_df['date'], first_df['actual'],
            label='å®Ÿç¸¾', linewidth=2, color='black', marker='o', markersize=3)

    # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
    colors = {'Prophet': '#ff6b6b', 'LightGBM': '#4dabf7'}
    for model_name, df in predictions.items():
        ax.plot(df['date'], df['prediction'],
                label=f'{model_name}äºˆæ¸¬', linewidth=2,
                linestyle='--', color=colors.get(model_name, 'gray'))

    ax.set_title('å®Ÿç¸¾ vs å„ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬', fontsize=14, fontweight='bold')
    ax.set_xlabel('æ—¥ä»˜')
    ax.set_ylabel('å£²ä¸Šï¼ˆå††ï¼‰')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{save_path}10_model_comparison_timeseries.png", dpi=150, bbox_inches='tight')
    plt.close()

    # === 2. è©•ä¾¡æŒ‡æ¨™ã®æ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰ ===
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    metrics = ['RMSE', 'MAE', 'MAPE', 'R2']
    colors = ['#ff6b6b', '#4dabf7']

    for idx, metric in enumerate(metrics):
        ax = axes[idx // 2, idx % 2]
        values = metrics_df[metric].values
        models = metrics_df['model'].values

        bars = ax.bar(models, values, color=colors)
        ax.set_title(f'{metric}', fontsize=14, fontweight='bold')

        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, val in zip(bars, values):
            if metric in ['RMSE', 'MAE']:
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                        f'Â¥{val:,.0f}', ha='center', va='bottom', fontsize=10)
            elif metric == 'MAPE':
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                        f'{val:.2f}%', ha='center', va='bottom', fontsize=10)
            else:
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),
                        f'{val:.4f}', ha='center', va='bottom', fontsize=10)

        # R2ã¯é«˜ã„ã»ã©è‰¯ã„ã€ä»–ã¯ä½ã„ã»ã©è‰¯ã„
        if metric == 'R2':
            ax.set_ylabel('ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰')
        else:
            ax.set_ylabel('èª¤å·®ï¼ˆä½ã„ã»ã©è‰¯ã„ï¼‰')

    plt.tight_layout()
    plt.savefig(f"{save_path}11_metrics_comparison.png", dpi=150, bbox_inches='tight')
    plt.close()

    # === 3. æ®‹å·®åˆ†æ ===
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    for idx, (model_name, df) in enumerate(predictions.items()):
        ax = axes[idx]
        residuals = df['actual'] - df['prediction']

        ax.scatter(df['prediction'], residuals, alpha=0.6,
                   color=colors[idx], s=30)
        ax.axhline(y=0, color='red', linestyle='--', linewidth=2)
        ax.set_title(f'{model_name} æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ', fontsize=14, fontweight='bold')
        ax.set_xlabel('äºˆæ¸¬å€¤')
        ax.set_ylabel('æ®‹å·®ï¼ˆå®Ÿç¸¾ - äºˆæ¸¬ï¼‰')
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(f"{save_path}12_residual_analysis.png", dpi=150, bbox_inches='tight')
    plt.close()

    # === 4. èª¤å·®ã®åˆ†å¸ƒ ===
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    for idx, (model_name, df) in enumerate(predictions.items()):
        ax = axes[idx]
        errors = df['actual'] - df['prediction']

        ax.hist(errors, bins=30, edgecolor='white', alpha=0.7, color=colors[idx])
        ax.axvline(x=0, color='red', linestyle='--', linewidth=2)
        ax.axvline(x=errors.mean(), color='orange', linestyle='--', linewidth=2,
                   label=f'å¹³å‡èª¤å·®: Â¥{errors.mean():,.0f}')
        ax.set_title(f'{model_name} èª¤å·®ã®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax.set_xlabel('èª¤å·®ï¼ˆå®Ÿç¸¾ - äºˆæ¸¬ï¼‰')
        ax.set_ylabel('é »åº¦')
        ax.legend()

    plt.tight_layout()
    plt.savefig(f"{save_path}13_error_distribution.png", dpi=150, bbox_inches='tight')
    plt.close()

    print(f"\nâœ… æ¯”è¼ƒã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ")


def analyze_by_segment(predictions: Dict[str, pd.DataFrame]) -> None:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã®åˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†æ")
    print("=" * 60)

    # æ›œæ—¥åˆ¥ã®èª¤å·®
    for model_name, df in predictions.items():
        df['day_of_week'] = df['date'].dt.dayofweek
        df['error'] = df['actual'] - df['prediction']
        df['abs_error'] = np.abs(df['error'])
        df['pct_error'] = np.abs(df['error'] / df['actual']) * 100

        print(f"\nã€{model_name}ã€‘æ›œæ—¥åˆ¥ å¹³å‡çµ¶å¯¾èª¤å·®ç‡:")
        dow_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        dow_errors = df.groupby('day_of_week')['pct_error'].mean()

        for dow, error in dow_errors.items():
            print(f"   {dow_names[dow]}æ›œæ—¥: {error:.2f}%")


def winner_summary(metrics_df: pd.DataFrame) -> None:
    """å‹è€…ã‚’ç™ºè¡¨"""
    print("\n" + "=" * 60)
    print("ğŸ† ç·åˆè©•ä¾¡")
    print("=" * 60)

    # å„æŒ‡æ¨™ã§ã®å‹è€…
    print("\nå„æŒ‡æ¨™ã§ã®ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«:")

    # RMSE, MAE, MAPEã¯ä½ã„ã»ã©è‰¯ã„
    for metric in ['RMSE', 'MAE', 'MAPE']:
        best_idx = metrics_df[metric].idxmin()
        best_model = metrics_df.loc[best_idx, 'model']
        best_value = metrics_df.loc[best_idx, metric]
        if metric in ['RMSE', 'MAE']:
            print(f"   {metric}: {best_model} (Â¥{best_value:,.0f})")
        else:
            print(f"   {metric}: {best_model} ({best_value:.2f}%)")

    # R2ã¯é«˜ã„ã»ã©è‰¯ã„
    best_idx = metrics_df['R2'].idxmax()
    best_model = metrics_df.loc[best_idx, 'model']
    best_value = metrics_df.loc[best_idx, 'R2']
    print(f"   RÂ²: {best_model} ({best_value:.4f})")

    # ç·åˆå‹è€…ï¼ˆR2ã§åˆ¤å®šï¼‰
    overall_best = metrics_df.loc[metrics_df['R2'].idxmax(), 'model']
    print(f"\nğŸ¥‡ ä»Šå›ã®ãƒ‡ãƒ¼ã‚¿ã§ã¯ã€{overall_best}ã€‘ãŒå„ªå‹¢ï¼")
    print("\nâ€»ãŸã ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ã‚ˆã£ã¦çµæœã¯å¤‰ã‚ã‚Šã¾ã™")
    print("   - èª¬æ˜æ€§ãŒé‡è¦ â†’ Prophetï¼ˆæˆåˆ†åˆ†è§£ãŒè¦‹ã‚„ã™ã„ï¼‰")
    print("   - ç²¾åº¦ãŒæœ€å„ªå…ˆ â†’ LightGBMï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æ¬¡ç¬¬ï¼‰")
    print("   - é•·æœŸäºˆæ¸¬ â†’ Prophetï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰æ•æ‰ãŒå¾—æ„ï¼‰")
    print("   - çŸ­æœŸäºˆæ¸¬ â†’ LightGBMï¼ˆãƒ©ã‚°ç‰¹å¾´é‡ãŒåŠ¹ãï¼‰")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ğŸ”¬ ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ & ç²¾åº¦è©•ä¾¡")
    print("=" * 60)

    # äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã¿
    predictions = load_predictions()

    # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ
    metrics_df = compare_models(predictions)

    # æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
    plot_comparison(predictions, metrics_df)

    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥åˆ†æ
    analyze_by_segment(predictions)

    # ç·åˆè©•ä¾¡
    winner_summary(metrics_df)

    # çµæœã‚’ä¿å­˜
    metrics_df.to_csv("model_comparison_results.csv", index=False)
    print("\nâœ… æ¯”è¼ƒçµæœã‚’ model_comparison_results.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")

    return metrics_df


if __name__ == "__main__":
    metrics_df = main()
