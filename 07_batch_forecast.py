"""
ãƒãƒƒãƒäºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã€œChronosã‚’å®šæœŸå®Ÿè¡Œã§æœ¬ç•ªé‹ç”¨ã™ã‚‹ã€œ

Usage:
    python 07_batch_forecast.py
    python 07_batch_forecast.py --model-size base --days 60
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import torch
from chronos import ChronosPipeline
import json
import logging
import argparse

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SalesForecastBatch:
    """å£²ä¸Šäºˆæ¸¬ãƒãƒƒãƒå‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        model_size: str = "small",
        prediction_days: int = 30,
        output_dir: str = "forecasts"
    ):
        """
        Parameters
        ----------
        model_size : str
            Chronosãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º: tiny, mini, small, base, large
        prediction_days : int
            äºˆæ¸¬æ—¥æ•°
        output_dir : str
            å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.model_size = model_size
        self.prediction_days = prediction_days
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        self.pipeline = None
        self.device = None

    def load_model(self):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        logger.info(f"ğŸ¤– Loading Chronos model (size={self.model_size})...")

        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        self.pipeline = ChronosPipeline.from_pretrained(
            f"amazon/chronos-t5-{self.model_size}",
            device_map=self.device,
            torch_dtype=torch.float32,
        )
        logger.info(f"   âœ… Model loaded on {self.device}")

    def load_data(self, data_path: str) -> pd.DataFrame:
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ“‚ Loading data from {data_path}...")

        df = pd.read_csv(data_path)
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if 'sales' not in df.columns:
            raise ValueError("'sales' column not found in data")

        if df['sales'].isnull().any():
            logger.warning("   âš ï¸ Found NaN values in sales, filling with forward fill")
            df['sales'] = df['sales'].fillna(method='ffill')

        logger.info(f"   âœ… Loaded {len(df)} records")
        logger.info(f"   ğŸ“… Period: {df['date'].min()} ~ {df['date'].max()}")

        return df

    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """äºˆæ¸¬ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸ”® Predicting next {self.prediction_days} days...")

        # tensorã«å¤‰æ›
        context = torch.tensor(df['sales'].values, dtype=torch.float32)

        # äºˆæ¸¬å®Ÿè¡Œ
        forecast = self.pipeline.predict(
            context,
            prediction_length=self.prediction_days,
            num_samples=20,
        )

        forecast_np = forecast.numpy()

        # äºˆæ¸¬æ—¥ä»˜ã‚’ç”Ÿæˆ
        last_date = df['date'].max()
        forecast_dates = pd.date_range(
            start=last_date + pd.Timedelta(days=1),
            periods=self.prediction_days,
            freq='D'
        )

        # çµ±è¨ˆé‡ã‚’è¨ˆç®—
        median = np.median(forecast_np, axis=1).squeeze()
        lower_95 = np.percentile(forecast_np, 2.5, axis=1).squeeze()
        upper_95 = np.percentile(forecast_np, 97.5, axis=1).squeeze()
        lower_50 = np.percentile(forecast_np, 25, axis=1).squeeze()
        upper_50 = np.percentile(forecast_np, 75, axis=1).squeeze()

        # çµæœã‚’DataFrameã«
        results = pd.DataFrame({
            'date': forecast_dates,
            'forecast': median.astype(int),
            'lower_95': lower_95.astype(int),
            'upper_95': upper_95.astype(int),
            'lower_50': lower_50.astype(int),
            'upper_50': upper_50.astype(int),
        })

        logger.info("   âœ… Prediction completed")

        # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        logger.info(f"   ğŸ“Š Forecast summary:")
        logger.info(f"      - Mean: Â¥{results['forecast'].mean():,.0f}")
        logger.info(f"      - Min:  Â¥{results['forecast'].min():,.0f}")
        logger.info(f"      - Max:  Â¥{results['forecast'].max():,.0f}")

        return results

    def save_results(self, results: pd.DataFrame, run_id: str = None):
        """çµæœã‚’ä¿å­˜"""
        if run_id is None:
            run_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # CSVä¿å­˜
        csv_path = self.output_dir / f"forecast_{run_id}.csv"
        results.to_csv(csv_path, index=False)
        logger.info(f"ğŸ’¾ Saved forecast to {csv_path}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            'run_id': run_id,
            'model': f'chronos-t5-{self.model_size}',
            'device': self.device,
            'prediction_days': self.prediction_days,
            'created_at': datetime.now().isoformat(),
            'forecast_start': results['date'].min().isoformat(),
            'forecast_end': results['date'].max().isoformat(),
            'forecast_mean': float(results['forecast'].mean()),
            'forecast_total': float(results['forecast'].sum()),
        }

        meta_path = self.output_dir / f"metadata_{run_id}.json"
        with open(meta_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ’¾ Saved metadata to {meta_path}")

        return csv_path, meta_path

    def run(self, data_path: str) -> pd.DataFrame:
        """ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
        start_time = datetime.now()

        logger.info("=" * 60)
        logger.info("ğŸš€ Starting batch forecast job")
        logger.info("=" * 60)

        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            self.load_model()

            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.load_data(data_path)

            # äºˆæ¸¬
            results = self.predict(df)

            # ä¿å­˜
            csv_path, meta_path = self.save_results(results)

            elapsed = (datetime.now() - start_time).total_seconds()

            logger.info("=" * 60)
            logger.info(f"âœ… Batch job completed successfully")
            logger.info(f"â±ï¸ Elapsed time: {elapsed:.1f} seconds")
            logger.info("=" * 60)

            return results

        except Exception as e:
            logger.error(f"âŒ Batch job failed: {e}")
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='Sales Forecast Batch Job')
    parser.add_argument(
        '--data', '-d',
        default='retail_sales_preprocessed.csv',
        help='Input data path'
    )
    parser.add_argument(
        '--model-size', '-m',
        default='small',
        choices=['tiny', 'mini', 'small', 'base', 'large'],
        help='Chronos model size'
    )
    parser.add_argument(
        '--days', '-n',
        type=int,
        default=30,
        help='Number of days to predict'
    )
    parser.add_argument(
        '--output-dir', '-o',
        default='forecasts',
        help='Output directory'
    )

    args = parser.parse_args()

    batch = SalesForecastBatch(
        model_size=args.model_size,
        prediction_days=args.days,
        output_dir=args.output_dir
    )

    results = batch.run(args.data)

    # çµæœã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    print("\nğŸ“‹ Forecast Preview:")
    print(results.head(10).to_string(index=False))


if __name__ == "__main__":
    main()
