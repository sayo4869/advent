"""
AWS ECSç”¨ãƒãƒƒãƒäºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã€œS3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾— â†’ äºˆæ¸¬ â†’ S3ã«ä¿å­˜ã€œ

ç’°å¢ƒå¤‰æ•°:
    S3_INPUT_BUCKET: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®S3ãƒã‚±ãƒƒãƒˆ
    S3_INPUT_KEY: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®S3ã‚­ãƒ¼
    S3_OUTPUT_BUCKET: å‡ºåŠ›å…ˆã®S3ãƒã‚±ãƒƒãƒˆ
    S3_OUTPUT_PREFIX: å‡ºåŠ›å…ˆã®S3ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    MODEL_SIZE: Chronosãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (tiny/mini/small/base/large)
    PREDICTION_DAYS: äºˆæ¸¬æ—¥æ•°
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import torch
from chronos import ChronosPipeline
import json
import logging
import boto3
from io import StringIO, BytesIO

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SalesForecastBatchAWS:
    """AWSç”¨å£²ä¸Šäºˆæ¸¬ãƒãƒƒãƒå‡¦ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’å–å¾—
        self.s3_input_bucket = os.environ.get('S3_INPUT_BUCKET', 'my-forecast-bucket')
        self.s3_input_key = os.environ.get('S3_INPUT_KEY', 'data/retail_sales_preprocessed.csv')
        self.s3_output_bucket = os.environ.get('S3_OUTPUT_BUCKET', 'my-forecast-bucket')
        self.s3_output_prefix = os.environ.get('S3_OUTPUT_PREFIX', 'forecasts/')
        self.model_size = os.environ.get('MODEL_SIZE', 'small')
        self.prediction_days = int(os.environ.get('PREDICTION_DAYS', '30'))

        self.s3_client = boto3.client('s3')
        self.pipeline = None
        self.device = None
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")

    def load_model(self):
        """Chronosãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰"""
        logger.info(f"ğŸ¤– Loading Chronos model (size={self.model_size})...")

        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠï¼ˆECSã§ã¯CPUãŒåŸºæœ¬ã€GPUä½¿ã†å ´åˆã¯cudaï¼‰
        if torch.cuda.is_available():
            self.device = "cuda"
        else:
            self.device = "cpu"

        self.pipeline = ChronosPipeline.from_pretrained(
            f"amazon/chronos-t5-{self.model_size}",
            device_map=self.device,
            torch_dtype=torch.float32,
        )
        logger.info(f"   âœ… Model loaded on {self.device}")

    def load_data_from_s3(self) -> pd.DataFrame:
        """S3ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        logger.info(f"ğŸ“‚ Loading data from s3://{self.s3_input_bucket}/{self.s3_input_key}...")

        response = self.s3_client.get_object(
            Bucket=self.s3_input_bucket,
            Key=self.s3_input_key
        )

        df = pd.read_csv(response['Body'])
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if 'sales' not in df.columns:
            raise ValueError("'sales' column not found in data")

        if df['sales'].isnull().any():
            logger.warning("   âš ï¸ Found NaN values, filling with forward fill")
            df['sales'] = df['sales'].fillna(method='ffill')

        logger.info(f"   âœ… Loaded {len(df)} records")
        logger.info(f"   ğŸ“… Period: {df['date'].min()} ~ {df['date'].max()}")

        return df

    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """äºˆæ¸¬ã‚’å®Ÿè¡Œ"""
        logger.info(f"ğŸ”® Predicting next {self.prediction_days} days...")

        context = torch.tensor(df['sales'].values, dtype=torch.float32)

        forecast = self.pipeline.predict(
            context,
            prediction_length=self.prediction_days,
            num_samples=20,
        )

        forecast_np = forecast.numpy()

        # äºˆæ¸¬æ—¥ä»˜ã‚’ç”Ÿæˆ
        last_date = df['date'].max()
        forecast_dates = pd.date_range(
            start=last_date + pd.Timedelta(days=1),
            periods=self.prediction_days,
            freq='D'
        )

        # çµ±è¨ˆé‡ã‚’è¨ˆç®—
        median = np.median(forecast_np, axis=1).squeeze()
        lower_95 = np.percentile(forecast_np, 2.5, axis=1).squeeze()
        upper_95 = np.percentile(forecast_np, 97.5, axis=1).squeeze()

        results = pd.DataFrame({
            'date': forecast_dates,
            'forecast': median.astype(int),
            'lower_95': lower_95.astype(int),
            'upper_95': upper_95.astype(int),
        })

        logger.info("   âœ… Prediction completed")
        logger.info(f"   ğŸ“Š Forecast mean: Â¥{results['forecast'].mean():,.0f}")

        return results

    def save_results_to_s3(self, results: pd.DataFrame):
        """çµæœã‚’S3ã«ä¿å­˜"""
        # CSVä¿å­˜
        csv_key = f"{self.s3_output_prefix}forecast_{self.run_id}.csv"
        csv_buffer = StringIO()
        results.to_csv(csv_buffer, index=False)

        self.s3_client.put_object(
            Bucket=self.s3_output_bucket,
            Key=csv_key,
            Body=csv_buffer.getvalue(),
            ContentType='text/csv'
        )
        logger.info(f"ğŸ’¾ Saved forecast to s3://{self.s3_output_bucket}/{csv_key}")

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        metadata = {
            'run_id': self.run_id,
            'model': f'chronos-t5-{self.model_size}',
            'device': self.device,
            'prediction_days': self.prediction_days,
            'created_at': datetime.now().isoformat(),
            'forecast_start': results['date'].min().isoformat(),
            'forecast_end': results['date'].max().isoformat(),
            'forecast_mean': float(results['forecast'].mean()),
            'forecast_total': float(results['forecast'].sum()),
            's3_input': f"s3://{self.s3_input_bucket}/{self.s3_input_key}",
            's3_output': f"s3://{self.s3_output_bucket}/{csv_key}",
        }

        meta_key = f"{self.s3_output_prefix}metadata_{self.run_id}.json"
        self.s3_client.put_object(
            Bucket=self.s3_output_bucket,
            Key=meta_key,
            Body=json.dumps(metadata, indent=2, ensure_ascii=False),
            ContentType='application/json'
        )
        logger.info(f"ğŸ’¾ Saved metadata to s3://{self.s3_output_bucket}/{meta_key}")

        # æœ€æ–°äºˆæ¸¬ã¸ã®ãƒã‚¤ãƒ³ã‚¿ã‚’æ›´æ–°ï¼ˆlatest.jsonï¼‰
        latest = {
            'latest_run_id': self.run_id,
            'latest_forecast': f"s3://{self.s3_output_bucket}/{csv_key}",
            'updated_at': datetime.now().isoformat(),
        }
        self.s3_client.put_object(
            Bucket=self.s3_output_bucket,
            Key=f"{self.s3_output_prefix}latest.json",
            Body=json.dumps(latest, indent=2),
            ContentType='application/json'
        )

        return csv_key, meta_key

    def run(self) -> dict:
        """ãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿè¡Œ"""
        start_time = datetime.now()

        logger.info("=" * 60)
        logger.info("ğŸš€ Starting AWS batch forecast job")
        logger.info(f"   Run ID: {self.run_id}")
        logger.info("=" * 60)

        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            self.load_model()

            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = self.load_data_from_s3()

            # äºˆæ¸¬
            results = self.predict(df)

            # S3ã«ä¿å­˜
            csv_key, meta_key = self.save_results_to_s3(results)

            elapsed = (datetime.now() - start_time).total_seconds()

            logger.info("=" * 60)
            logger.info(f"âœ… Batch job completed successfully")
            logger.info(f"â±ï¸ Elapsed time: {elapsed:.1f} seconds")
            logger.info("=" * 60)

            # Step Functionsç”¨ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
            return {
                'statusCode': 200,
                'run_id': self.run_id,
                'forecast_count': len(results),
                'forecast_mean': float(results['forecast'].mean()),
                's3_output': f"s3://{self.s3_output_bucket}/{csv_key}",
                'elapsed_seconds': elapsed,
            }

        except Exception as e:
            logger.error(f"âŒ Batch job failed: {e}")
            return {
                'statusCode': 500,
                'error': str(e),
                'run_id': self.run_id,
            }


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    batch = SalesForecastBatchAWS()
    result = batch.run()

    # çµæœã‚’æ¨™æº–å‡ºåŠ›ï¼ˆStep Functionsã§å–å¾—å¯èƒ½ï¼‰
    print(json.dumps(result, indent=2))

    # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1
    if result.get('statusCode') != 200:
        exit(1)


if __name__ == "__main__":
    main()
